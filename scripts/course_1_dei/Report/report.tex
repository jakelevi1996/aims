\documentclass{article}

\usepackage[a4paper, total={7in, 10in}]{geometry}

\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{graphicx}
\graphicspath{{../Results/Protected/}}

\title{AIMS Course 1: Data, Estimation and Inference}
\author{Jake Levi}
\date{October 2022}

\begin{document}
\maketitle
\section{Introduction}

This lab report investigates the use of Gaussian Processes, a type of machine learning model motivated by Bayesian probability theory, for modelling a meteorological dataset called Sotonmet. In a Gaussian Process model, given a mean function $\mu$, kernel function $K$, training input data $x$ (represented as a vector), and prediction inputs $x^*$, the noisy training labels $y$ (which we assume are noisy observations of unknown noiseless training labels $f$) and noiseless prediction labels $f^*$ have a joint Gaussian distribution:

\begin{equation}
    p\left( \begin{bmatrix}
        y \\
        f^*
    \end{bmatrix} \right)
    = \mathcal{N} \left( \begin{bmatrix}
        y \\
        f^*
    \end{bmatrix} \middle| \begin{bmatrix}
        \mu(x) \\
        \mu(x^*)
    \end{bmatrix}, \begin{bmatrix}
        K(x, x) + \sigma^2 I & K(x, x^*) \\
        K(x, x^*)^T & K(x^*, x^*) \\
    \end{bmatrix} \right)
\end{equation}

Where $K(x, x^*)$ is a matrix whose $(i, j)$th element is given by $K(x, x^*)_{i,j} = K(x_i, x^*_j)$. The predictive distribution $p(f^* \mid y)$ follows from the formula for the conditional distribution of a jointly Gaussian random variable \cite{bishop2006pattern}:

\begin{align}
    p(f^* \mid y) &= \mathcal{N}\left(f^* \mid \mu^*, \Sigma^* \right) \\
    \text{where} \qquad \mu^* &= \mu(x^*) + K(x^*, x) \left( K(x, x) + \sigma^2 I \right)^{-1} (y - \mu(x)) \\
    \Sigma^* &= K(x^*, x^*) - K(x^*, x) \left( K(x, x) + \sigma^2 I \right) ^{-1} K(x, x^*)
\end{align}

The log marginal likelihood (LML) of the noisy training labels $y$ given training input data $x$ (and also implicitly given any hyperparameters of the model) is given by:

\begin{align}
    \log \left( p(y \mid x) \right) = -\frac{1}{2}\log\left(\det \left(2\pi\left( K(x, x) + \sigma^2 I \right)\right)\right) -\frac{1}{2}(y - \mu(x))^T \left( K(x, x) + \sigma^2 I \right)^{-1} (y - \mu(x))
\end{align}

This expression implies that maximising the LML encourages $\mu(x)$, $K(x,x)$ and $\sigma$ to fit the data accurately and with calibrated uncertainty. Maximising the LML can also be motivated from a Bayesian perspective, which is discussed further in appendix \ref{appendix:why_lml}.

The focus in this coursework submission is on predicting the tide height given the time of day, for which the the training and ground truth data is shown in \ref{fig:data}, alongside an independent Gaussian Process prediction in \ref{fig:ind_pres}.

\appendix

\section{Figures}\label{appendix:figures}

\begin{figure}[ht]
    \centering
    \subfloat[
        \centering Training and ground truth data
        \label{fig:data}
    ]{
        \includegraphics[width=0.45\textwidth]{Sotonmet_data.png}
    }
    \subfloat[
        \centering Independent Gaussian Process predictions
        \label{fig:ind_pres}
    ]{
        \includegraphics[width=0.45\textwidth]{Data_and_independent_GP_predictions.png}
    }
    \caption{The Sotonmet dataset}
    \label{fig:RBF_manual}
\end{figure}


\section{Motivation for maximising the log marginal likelihood}\label{appendix:why_lml}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
