To quote Alex Kendall and Yarin Gal in \cite{kendall2017uncertainties}:

\begin{quote}
    There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data.
\end{quote}

We can model the performance of a GP in the presence of epistemic or aleatoric uncertainty by either removing a subsection of the data or by artificially adding noise to a subsection of the data respectively. In the case of sqe\_opt (which was optimised to have high LML), the results of one such experiment are shown in figure \ref{fig:uncertainty}.

Although this GP performs well in the presence of epistemic uncertainty, reverting to a larger predictive standard deviation when far from the vicinity of any training data, we see that this GP does not perform well in the presence of aleatoric uncertainty, making confidently wrong predictions (predictions which are far away from the ground truth labels and with high certainty/low standard deviation) in the vicinity of training data which has a high degree of noise.

We can understand this behaviour by looking at the expression for the predictive variance of a GP in equation \ref{eq:conditional variance}, which depends only on the input locations of the training data and predictions, and not on the labels of the training data. Of course, the predictive variance of sqe\_opt considered here depends indirectly on the training labels, as a result of its hyperparameters having been optimised with respect to the LML of the training data, however this model has no capacity to increase its predictive uncertainty in the presence of new noisy training labels.

This could be a very undesirable property for the model to have in a safety-critical prediction context, for example if one of the input sensors failed and started producing very noisy measurements, we would not want the model to produce wildly incorrect predictions with a high degree of certainty, rather we would prefer the model to increase its predictive uncertainty to fit the uncertainty in the newly observed data. One possible solution to this problem would be to model the observation noise (which does directly affect the predictive uncertainty of a GP) using the output from a second GP, which predicts observation noise as a function of the same input data as the original GP, and conditions on the noise of the training data, however we leave this as a direction for future work.
