We consider optimisation problems consisting of a decision variable $x$, which takes values in a set $\mathcal{X}$ (referred to as the domain of $x$), an objective function $f_0(x)$, inequality constraints $f_i(x)$ (for $i\in\{1, \hdots, m\}$), and equality constraints $h_i(x)$ (for $i\in\{1, \hdots, p\}$), which can be expressed in` the following form:
\begin{equation}
\begin{aligned}
    \underset{x \in \mathcal{X}}{\text{Minimise}} \quad & f_0(x) \\
    \text{Subject to} \quad & f_i(x) \le 0 \quad i\in\{1, \hdots, m\} \\
    & h_i(x) = 0 \quad i\in\{1, \hdots, p\}
\end{aligned} \label{eq:optimisation problem}
\end{equation}
We refer to the set of values of $x\in\mathcal{X}$ which satisfy the equality and inequality constraints as the feasible set, which is denoted by $\mathcal{F}$:
\begin{equation}
    \mathcal{F} = \{ x\in\mathcal{X}: (\forall i\in\{1, \hdots, m\}) \quad f_i(x) \le 0 \quad \text{and} \quad (\forall i\in\{1, \hdots, p\}) \quad h_i(x) = 0 \}
\end{equation}
The limit of the smallest value of $f_0(x)$ for any value of $x$ in the feasible set $\mathcal{F}$ is referred to as the optimal cost, and denoted by $p^*$:
\begin{equation}
    p^* = \underset{x\in\mathcal{F}}{\inf}\left[f_0(x)\right]
\end{equation}
When solving an optimisation problem in the form described by equation \ref{eq:optimisation problem}, it is useful to introduce the Lagrangian function \cite{boyd2004convex} (intuition for the form of the Lagrangian function is provided in appendix \ref{appendix:why lagrangian}), which is a function of the decision variable $x\in\mathcal{X}$, and also the variables $\lambda \in \mathbb{R}^m $ and $\nu\in\mathbb{R}^p$, which are referred to as the Lagrange multipliers for the inequality and equality constraints respectively:
\begin{equation}
    \mathcal{L}(x, \lambda, \nu) = f_0(x) + \sum_{i=1}^{m}[\lambda_i f_i(x)] + \sum_{i=1}^{p}[\nu_i h_i(x)] \label{eq:Lagrangian}
\end{equation}
The limit of the smallest value of the Lagrangian function for any value of $x$ in its domain $\mathcal{X}$ (not only in the feasible set $\mathcal{F}$) as a function of the Lagrange multipliers $\lambda$ and $\nu$ is referred to as the Lagrange dual function, denoted by $g$:
\begin{align}
    g(\lambda, \nu) &= \underset{x\in\mathcal{X}}{\inf}\left[\mathcal{L}(x, \lambda, \nu)\right] \label{eq:dual function} \\
    &= \underset{x\in\mathcal{X}}{\inf}\left[f_0(x) + \sum_{i=1}^{m}[\lambda_i f_i(x)] + \sum_{i=1}^{p}[\nu_i h_i(x)]\right]
\end{align}
The dual function is concave with respect to $\lambda$ and $\nu$, which is equivalent to the following statements (which are proved in appendix \ref{appendix:proof dual concave}):
\begin{align}
    (\forall\lambda^{(1)},\lambda^{(2)}\in\mathbb{R}^m)(\forall\alpha\in[0, 1]) \quad g(\alpha\lambda^{(1)} + (1 - \alpha)\lambda^{(2)}, \nu) &\ge \alpha g(\lambda^{(1)}, \nu) + (1 - \alpha) g(\lambda^{(2)}, \nu) \label{eq:dual concave lambda} \\
    (\forall\nu^{(1)},\nu^{(2)}\in\mathbb{R}^p)(\forall\alpha\in[0, 1]) \quad g(\lambda, \alpha\nu^{(1)} + (1 - \alpha)\nu^{(2)}) &\ge \alpha g(\lambda, \nu^{(1)}) + (1 - \alpha) g(\lambda, \nu^{(2)})\label{eq:dual concave nu}
\end{align}

We also note that for any $\lambda \ge 0$ (where $\ge$ is understood to apply element-wise to each element of $\lambda$) and for any $\nu$, the Lagrangian dual function $g(\lambda, \nu)$ is a lower bound for the optimal cost $p^*$. To prove this, it is useful to note that for any feasible point $\tilde{x}\in\mathcal{F}$ (which necessarily satisfies $f_i(x) \le 0$ for all $i\in\{1, \hdots, m\}$ and $h_i(x) = 0$ for all $i\in\{1, \hdots, p\}$), and for any $\lambda\ge 0$ and any $\nu$, the following inequality holds:
\begin{align}
    (\forall \tilde{x} \in \mathcal{F}, \lambda \ge 0, \nu) \quad 0 &\ge \sum_{i=1}^{m}[\lambda_i f_i(\tilde{x})] + \sum_{i=1}^{p}[\nu_i h_i(\tilde{x})] \\
    \Rightarrow (\forall \tilde{x} \in \mathcal{F}, \lambda \ge 0, \nu) \quad f_0(\tilde{x}) &\ge f_0(\tilde{x}) + \sum_{i=1}^{m}[\lambda_i f_i(\tilde{x})] + \sum_{i=1}^{p}[\nu_i h_i(\tilde{x})] \\
    &= \mathcal{L}(\tilde{x}, \lambda, \nu) \\
    &\ge \underset{x\in\mathcal{X}}{\inf}\left[\mathcal{L}(x, \lambda, \nu)\right] \\
    &= g(\lambda, \nu)
\end{align}
Since the Lagrangian dual function is a lower bound on the objective function for any value of $\tilde{x}$ in the feasible set $\mathcal{F}$, it is also true for the value of $\tilde{x} \in \mathcal{F}$ which minimises the objective function, $f_0(x)$:
\begin{align}
    (\forall \tilde{x} \in \mathcal{F}, \lambda \ge 0, \nu) \quad g(\lambda, \nu) &\le f_0(\tilde{x}) \\
    \Rightarrow (\forall \lambda \ge 0, \nu) \quad g(\lambda, \nu) &\le \underset{x\in\mathcal{F}}{\inf}\left[f_0(x)\right] \\
    &= p^*
\end{align}
The limit of the greatest value of $g(\lambda, \nu)$ for any $\lambda \ge 0$ and $\nu$ is the best lower bound on $p^*$, and is referred to as the dual optimal cost, denoted by $d^*$:
\begin{align}
    d^* &= \underset{\lambda \ge 0, \nu}{\sup}\left[g(\lambda, \nu)\right] \\
    &= \underset{\lambda \ge 0, \nu}{\sup}\left[\underset{x\in\mathcal{X}}{\inf}\left[\mathcal{L}(x, \lambda, \nu)\right]\right] \\
    & \le p^*
\end{align}
Therefore we have that the dual optimal cost $d^*$ is always less than or equal to the primal optimal cost, $p^*$. An interesting symmetry exists between the optimal cost $p^*$ (also referred to as the primal optimal cost) and the dual optimal cost, $d^*$. To see this, we start by noting that the Lagrangian function $\mathcal{L}(x, \lambda, \nu)$ is unbounded above (and below) with respect to $\lambda$ and $\nu $ unless $x$ is in the feasible set $\mathcal{F}$, in which case the Lagrangian function is bounded above by the objective function, $f_0(x)$:
\begin{align}
    \underset{\lambda \ge 0, \nu}{\sup}\left[ \mathcal{L}(x, \lambda, \nu) \right] &= \begin{cases}
        f_0(x) & x \in \mathcal{F} \\
        \infty & \text{otherwise}
    \end{cases} \\
    \Rightarrow \underset{x\in\mathcal{X}}{\inf}\left[\underset{\lambda \ge 0, \nu}{\sup}\left[\mathcal{L}(x, \lambda, \nu)\right]\right] &= \underset{x\in\mathcal{F}}{\inf}\left[f_0(x)\right] \\
    &= p^* \\
    \Rightarrow \underset{\lambda \ge 0, \nu}{\sup}\left[\underset{x\in\mathcal{X}}{\inf}\left[\mathcal{L}(x, \lambda, \nu)\right]\right] &\le \underset{x\in\mathcal{X}}{\inf}\left[\underset{\lambda \ge 0, \nu}{\sup}\left[\mathcal{L}(x, \lambda, \nu)\right]\right]
\end{align}
This latter inequality is a special case of the Maxâ€“min inequality, and in the context of optimisation, it is known as the principle of weak duality. In cases where equality holds, and the dual optimal cost is equal to the primal optimal cost, $d^* = p^*$, this is known as strong duality.

The Lagrangian and Lagrangian dual functions are therefore useful in constrained optimisation, because they allow us to minimise the Lagrangian function $\mathcal{L}$ with respect to $x$ as a function of $\lambda$ and $\nu$ without constraints (which yields the Langrangian dual function $g$), and then to maximise the Langrangian dual function $g$ (which is necessarily concave) with respect to $\lambda$ and $\nu$, subject only to the constraints that each element of $\lambda$ is nonnegative (if the original problem has no inequality constraints then there are no constraints on the maximisation of $g$ at all), which yields the dual optimal cost $d^*$. The dual optimal cost provides a lower bound on the value $p^*$ in all cases, and in the case of strong duality, provides the value of $p^*$ itself.
