\subsection{Standard Form Of Convex Problems}
A convex optimisation problem is one in which the objective function $f_0(x)$ (defined in equation \ref{eq:optimisation problem}) is a convex function, and the feasible set $\mathcal{F}$ (defined in equation \ref{eq:feasible set}) is a convex set. In order for $\mathcal{F}$ to be a convex set, any inequality constraints must restrict $x$ to be in a convex subset of its domain, and any equality constraints must restrict $x$ to be in an affine subspace of its domain. Therefore, a convex optimisation problem in the form of equation \ref{eq:optimisation problem} is said to be a standard form convex optimisation problem if the objective function $f_0(x)$ is a convex function, all inequality constraint functions $f_i(x)$ are convex functions ($\forall i\in\{1, \hdots, m\}$), and all equality constraint functions $h_i(x)$ are affine functions ($\forall i\in\{1, \hdots, p\}$), in which case the problem is a convex optimisation problem. However, not all convex optimisation problems are in standard form, as the following example demonstrates:
\begin{equation}
\begin{aligned}
    \underset{x \in \mathbb{R}^2}{\text{Minimise}} \quad & x_1^2 + x_2^2 \\
    \text{Subject to} \quad & f_1(x) = \frac{x_1}{1 + x_2^2} \le 0 \\
    & h_1(x) = (x_1 + x_2)^2 = 0
\end{aligned} \label{eq:non standard convex problem}
\end{equation}
In this case, $f_1(x)$ is not a convex function, and $h_1(x)$ is not an affine function, therefore the problem is not a standard form convex optimisation problem, however the objective function is a convex function, and the feasible set defined by these constraints is a convex set:
\begin{align*}
    \frac{x_1}{1 + x_2^2} &\le 0 \\
    (\forall x_2 \in \mathbb{R}) \quad \frac{1}{1 + x_2^2} &> 0 \\
    \Rightarrow x_1 &\le 0 \\
    (x_1 + x_2)^2 &= 0 \\
    \Rightarrow x_1 + x_2 &= 0 \\
    \Rightarrow x_2 &= -x_1 \\
    \Rightarrow x_2 &\ge 0 \\
    \Rightarrow \mathcal{F} &= \{ x\in\mathbb{R}^2: x_1 \le 0 \quad \text{and} \quad x_2 = -x_1 \}
\end{align*}
Therefore the problem in equation \ref{eq:non standard convex problem} is equivalent to the following standard form convex optimisation problem (for which the primal optimal cost is clearly $0$, achieved when $x_1=x_2=0$):
\begin{align*}
    \underset{x \in \mathbb{R}^2}{\text{Minimise}} \quad & x_1^2 + x_2^2 \\
    \text{Subject to} \quad & f_1(x) = x_1 \le 0 \\
    & h_1(x) = x_1 + x_2 = 0
\end{align*}

\subsection{Hyperbolic Constraints And Second-Order Cones}
A second-order cone problem (SOCP) is an optimisation problem that has the following form (see \cite{boyd2004convex}, equation 4.36, page 156):
\begin{equation}
\begin{aligned}
    \underset{x \in \mathcal{X}}{\text{Minimise}} \quad & f^Tx \\
    \text{Subject to} \quad & \Vert A_ix + b_i \Vert_2 \le c_i^Tx + d_i \quad i\in\{1, \hdots, m\} \\
    & Fx = g
\end{aligned} \label{eq:SOCP}
\end{equation}
The following equivalence can be used to express several different types of problems as SOCPs, which holds for any $x \in \mathbb{R}^n$ and $y, z \in \mathbb{R}$ which satisfy $y>0$ and $z>0$:
\begin{equation}
    x^Tx \le yz \quad \Leftrightarrow \quad \left\Vert\begin{bmatrix}
        2x \\
        y - z
    \end{bmatrix}\right\Vert_2 \le y + z \label{eq:SOCP identity}
\end{equation}
This equivalence can be proved as follows:
\begin{align*}
    && \left\Vert\begin{bmatrix}
        2x \\
        y - z
    \end{bmatrix}\right\Vert_2 &= \sqrt{\begin{bmatrix}
        2x \\
        y - z
    \end{bmatrix}^T \begin{bmatrix}
        2x \\
        y - z
    \end{bmatrix}} \\
    && &= \sqrt{(2x)^T(2x) + (y-z)^2} \\
    && &= \sqrt{4x^T x + y^2 - 2yz + z^2} \\
    && 0 \le \left\Vert\begin{bmatrix}
        2x \\
        y - z
    \end{bmatrix}\right\Vert_2 &\le y + z \\
    \Leftrightarrow && 0 \le \sqrt{4x^T x + y^2 - 2yz + z^2} &\le y + z \\
    \Leftrightarrow && 4x^T x + y^2 - 2yz + z^2 &\le y^2 + 2yz + z^2 \\
    \Leftrightarrow && 4x^T x &\le 4yz \\
    \Leftrightarrow && x^T x &\le yz
\end{align*}
This equivalence can be used for example to show that the following optimisation problem, which can be interpreted as maximising a harmonic mean, is equivalent to a SOCP ($a_i^T$ refers to the $i$th row of $A$):
\begin{equation}
\begin{aligned}
    \underset{x}{\text{Maximise}} \quad & \left( \sum_{i=1}^{m}\left[ \frac{1}{a_i^T x - b_i} \right] \right)^{-1} \\
    \text{Subject to} \quad & Ax > b
\end{aligned}
\end{equation}
This equivalence can be demonstrated as follows ($\cong$ is used to denote equivalence between optimisation problems, $\textbf{0}$ in bold-face is used to denote an appropriately sized matrix or vector in which every element is equal to 0, $\textbf{1}$ in bold-face is used to denote an appropriately sized vector in which every element is equal to 1, and $e_i$ is used to denote the $i$th basis vector in which the $i$th element is equal to 1 and all other elements are equal to 0):
\begin{align*}
    \underset{x}{\text{Maximise}} \quad & \left( \sum_{i=1}^{m}\left[ \frac{1}{a_i^T x - b_i} \right] \right)^{-1} \\
    \text{Subject to} \quad & Ax > b \\ \\
    \cong \quad \underset{x}{\text{Minimise}} \quad & \sum_{i=1}^{m}\left[ \frac{1}{a_i^T x - b_i} \right] \\
    \text{Subject to} \quad & Ax > b \\ \\
    \cong \quad \underset{x, y}{\text{Minimise}} \quad & \textbf{1}^T y \\
    \text{Subject to} \quad & Ax > b \\
    & y_i(a_i^T x - b_i) = 1 \quad (\forall i) \\ \\
    \cong \quad \underset{x, y}{\text{Minimise}} \quad & \textbf{1}^T y \\
    \text{Subject to} \quad & y \ge 0 \\
    & y_i(a_i^T x - b_i) \ge 1 \quad (\forall i) \\ \\
    \cong \quad \underset{x, y}{\text{Minimise}} \quad & \textbf{1}^T y \\
    \text{Subject to} \quad & \Vert 0 \Vert_2 \le y_i \quad (\forall i) \\
    & \left\Vert\begin{bmatrix}
        2 \\
        y_i - a_i^T x + b_i
    \end{bmatrix}\right\Vert_2 \le y_i + a_i^T x - b_i \quad (\forall i) \\ \\
    \cong \quad \underset{x, y}{\text{Minimise}} \quad & \begin{bmatrix} \mathbf{0} \\ \mathbf{1} \end{bmatrix}^T \begin{bmatrix} x \\ y \end{bmatrix} \\
    \text{Subject to} \quad & \left\Vert\begin{bmatrix} \mathbf{0} & \mathbf{0} \\ \mathbf{0} & \mathbf{0} \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix} \right\Vert_2 \le \begin{bmatrix} \mathbf{0} \\ e_i \end{bmatrix}^T \begin{bmatrix} x \\ y \end{bmatrix} + 0 \quad (\forall i) \\
    & \left\Vert\begin{bmatrix} \mathbf{0} & \mathbf{0} \\ -a_i^T & e_i^T \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} 2 \\ b_i \end{bmatrix} \right\Vert_2 \le \begin{bmatrix} a_i \\ e_i \end{bmatrix}^T \begin{bmatrix} x \\ y \end{bmatrix} - b_i \quad (\forall i)
\end{align*}
Which at last is in the form of a SOCP defined in equation \ref{eq:SOCP}.

The equivalence in equation \ref{eq:SOCP identity} can also be used to show that a similar problem which can be interpreted as maximising a geometric mean can be reformulated as a SOCP (assuming $m$ is a power of 2):
\begin{equation}
\begin{aligned}
    \underset{x}{\text{Maximise}} \quad & \left( \prod_{i=1}^{m}\left[ a_i^T x - b_i \right] \right)^{1/m} \\
    \text{Subject to} \quad & Ax > b
\end{aligned} \label{eq:geometric mean SOCP}
\end{equation}
The trick is to recursively define variables $t_{j, i}$ for $j\in\{1,\hdots,\log_2(m)\}$ as follows:
\begin{align*}
    t_{0, i} &= a_i^T x - b_i \\
    t_{j+1, i} &= \sqrt{(t_{j, 2i})(t_{j, 2i+1})} \\
    \Rightarrow \quad t_{j+1, i}^2 &\le (t_{j, 2i})(t_{j, 2i+1}) \\
    \Leftrightarrow \quad \left\Vert\begin{bmatrix}
        2t_{j+1, i} \\
        t_{j, 2i} - t_{j, 2i+1}
    \end{bmatrix}\right\Vert_2 &\le t_{j, 2i} + t_{j, 2i+1}
\end{align*}
The optimisation problem in equation \ref{eq:geometric mean SOCP} can therefore be expressed as an equivalent SOCP as follows:
\begin{equation}
\begin{aligned}
    \underset{x}{\text{Maximise}} \quad & t_{\log_2(m), 1} \\
    \text{Subject to} \quad & t_{0, i} = a_i^T x - b_i \quad & (\forall i) \\
    & t_{j, i} \ge 0 \quad & (\forall i)(\forall j \in \{0, \hdots, \log_2(m)\}) \\
    & \left\Vert\begin{bmatrix}
        2t_{j+1, i} \\
        t_{j, 2i} - t_{j, 2i+1}
    \end{bmatrix}\right\Vert_2 \le t_{j, 2i} + t_{j, 2i+1} \quad & (\forall i)(\forall j \in \{0, \hdots, \log_2(m-1)\}) \\
\end{aligned}
\end{equation}

\subsection{Support Functions}
\subsection{Largest-L Norm Of A Vector}
