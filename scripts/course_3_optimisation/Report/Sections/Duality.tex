\subsection{Projection Onto The L1 Ball}
A point $a \in \mathbb{R}^N$ can be projected onto the unit ball in the $\ell_1$ norm by solving the following quadratic program:
\begin{align*}
    \underset{x}{\text{Minimise}} \quad & \frac{1}{2}\Vert x - a \Vert_2^2 \\
    \text{Subject to} \quad & \Vert x \Vert_1 \le c
\end{align*}
This is equivalent to the following quadratic program with slack variables $t$ introduced:
\begin{align*}
    \underset{x,t}{\text{Minimise}} \quad & \frac{1}{2}(x - a)^T(x - a) \\
    \text{Subject to} \quad & x \le t \\
    & x \ge -t \\
    & \mathbf{1}^T t \le c
\end{align*}
The Lagrangian and Lagrangian dual functions can be defined for this problem as follows:
\begin{align*}
    \mathcal{L}(x, t, \lambda) &= \frac{1}{2}(x - a)^T(x - a) + \lambda_1^T(x - t) + \lambda_2^T(-x - t) + \lambda_3(\mathbf{1}^T t - c) \\
    &= \frac{1}{2}x^Tx + (-a + \lambda_1 - \lambda_2)^T x + \frac{1}{2}a^Ta + (-\lambda_1 - \lambda_2 + \lambda_3\mathbf{1})^Tt - \lambda_3c \\
    \frac{\partial\mathcal{L}}{\partial x} &= x + (-a + \lambda_1 - \lambda_2) \\
    \frac{\partial\mathcal{L}}{\partial x} = 0 \quad &\Rightarrow \quad x = a - \lambda_1 + \lambda_2 \\
    &\Rightarrow \quad \mathcal{L}(x, t, \lambda) = -\frac{1}{2}(a - \lambda_1 + \lambda_2)^T(a - \lambda_1 + \lambda_2) + \frac{1}{2}a^Ta + (-\lambda_1 - \lambda_2 + \lambda_3\mathbf{1})^Tt - \lambda_3c \\
    g(\lambda) &= \underset{x, t}{\inf}\left[\mathcal{L}(x, t, \lambda)\right] \\
    &= \begin{cases}
        -\frac{1}{2}(a - \lambda_1 + \lambda_2)^T(a - \lambda_1 + \lambda_2) + \frac{1}{2}a^Ta - \lambda_3c \quad & -\lambda_1 - \lambda_2 + \lambda_3\mathbf{1} = 0 \\
        -\infty & \text{Otherwise}
    \end{cases}
\end{align*}
The dual problem in this case is equivalent to maximising the dual function $g(\lambda)$ with respect to $\lambda\ge0$, which can be expressed as the following quadratic program:
\begin{align*}
    \underset{\lambda_1, \lambda_2, \lambda_3}{\text{Maximise}} \quad & -\frac{1}{2}(a - \lambda_1 + \lambda_2)^T(a - \lambda_1 + \lambda_2) + \frac{1}{2}a^Ta - \lambda_3c \\
    \text{Subject to} \quad & -\lambda_1 - \lambda_2 + \lambda_3\mathbf{1} = 0 \\
    & \lambda_1 \ge 0 \\
    & \lambda_2 \ge 0 \\
    & \lambda_3 \ge 0
\end{align*}
The dual problem could be solved efficiently by using an interior point method, for example. From the solution to the dual problem, the optimal value for $x$ can be found easily as the value that minimises the Lagrangian function, where $\lambda_1$ and $\lambda_2$ represent the Lagrange multipliers for the inequality constraints $x \le t$ and $x \ge -t$ respectively:
\begin{equation*}
    x = a - \lambda_1 + \lambda_2
\end{equation*}

\subsection{SVM Duality}
A standard support vector machine problem can be expressed as follows:
\begin{equation*}
    \min_{a, b}{\left[ \sum_i{\Bigl[ \max{ \Bigl[ \{ 0, 1-s_i(a^Tv_i + b) \} \Bigl] } \Bigl]} + k\Vert a \Vert_2^2 \right]}
\end{equation*}
This problem can be expressed as a quadratic program by introducing the slack variables $t$, matrix $V$, and diagonal matrix $S$ as follows:
\begin{align*}
    V_{ij} = &(v_i)_j \\
    S_{ij} = &\begin{cases}
        s_i & i = j \\
        0 & i \ne j
    \end{cases} \\
    \Rightarrow \quad \underset{a, b, t}{\text{Maximise}} \quad & \mathbf{1}^T t + k a^T a \\
    & t \ge 0 \\
    & t \ge \mathbf{1} - S(Va + b\mathbf{1})
\end{align*}
The Lagrangian and Lagrangian dual functions can be defined for this problem as follows:
\begin{align*}
    \mathcal{L}(a, b, t, \lambda) &= \mathbf{1}^T t + k a^T a - \lambda_1^Tt + \lambda_2^T(\mathbf{1} - S(Va + b\mathbf{1}) - t) \\
    &= (\mathbf{1} - \lambda_1 - \lambda_2)^T t + ka^T a - \lambda_2^TSVa - \lambda_2^TS\mathbf{1}b + \lambda_2^T\mathbf{1} \\
    \frac{\partial\mathcal{L}}{\partial a} &= 2ka - V^TS\lambda_2 \\
    \frac{\partial\mathcal{L}}{\partial a} = 0 \quad &\Rightarrow \quad a = \frac{1}{2k}V^TS\lambda_2 \\
    &\Rightarrow \quad \mathcal{L}(a, b, t, \lambda) = (\mathbf{1} - \lambda_1 - \lambda_2)^T t -\frac{1}{4k}\lambda_2^TSVV^TS\lambda_2 - \lambda_2^TS\mathbf{1}b + \lambda_2^T\mathbf{1} \\
    g(\lambda) &= \underset{a, b, t}{\inf}\left[\mathcal{L}(a, b, t, \lambda)\right] \\
    &= \begin{cases}
        -\frac{1}{4k}\lambda_2^TSVV^TS\lambda_2 + \lambda_2^T\mathbf{1} \quad & \mathbf{1} - \lambda_1 - \lambda_2 = 0 \quad \text{and} \quad \lambda_2^TS\mathbf{1} = 0 \\
        -\infty & \text{Otherwise}
    \end{cases}
\end{align*}
The dual problem in this case is equivalent to maximising the dual function $g(\lambda)$ with respect to $\lambda\ge0$, which can be expressed as the following quadratic program:
\begin{align*}
    \underset{\lambda_1, \lambda_2, \lambda_3}{\text{Maximise}} \quad & -\frac{1}{4k}\lambda_2^TSVV^TS\lambda_2 + \lambda_2^T\mathbf{1} \\
    \text{Subject to} \quad & \mathbf{1} - \lambda_1 - \lambda_2 = 0 \\
    & \lambda_2^TS\mathbf{1} = 0 \\
    & \lambda_1 \ge 0 \\
    & \lambda_2 \ge 0 \\
    & \lambda_3 \ge 0
\end{align*}

\subsection{Adjustable Optimization}
In an uncertain optimisation problem, a decision $x$ is made, after which a vector $w$ is observed, which initially is only known to lie in the set $\mathcal{W} = \{ w: Fw \le \mathbf{1} \}$, and the decision is then updated to $x + f(w)$. To simplify the problem, it can be assumed that $f(w)$ is linear, which is to say that $f(w) = Mw$ for some matrix $M$, and the problem becomes choosing both $x$ and $M$ to minimise some worst-case cost, depending on the (initially unknown, but later observed) value of $w$. This scenario can be expressed as the following optimisation problem:
\begin{align*}
    \underset{M, x}{\text{Minimise}} \quad & \underset{w\in\mathcal{W}}{\max}\Bigl[ c^T(x + Mw) \Bigr] \\
    \text{Subject to} \quad & A(x + Mw) \le b + Bw \quad (\forall w\in\mathcal{W})
\end{align*}
An upper bound on the worst-case cost can be found using the Lagrangian dual function of the following problem:
\begin{align*}
    \underset{w}{\text{Maximise}} \quad & c^T(x + Mw) \\
    \text{Subject to} \quad & Fw \le \mathbf{1}
\end{align*}
The Lagrangian and Langrangian dual functions for this problem can be defined as follows:
\begin{align*}
    \mathcal{L}(w, \lambda) &= c^T(x + Mw) + \lambda^T(Fw - \mathbf{1}) \\
    g(\lambda) &= \underset{w}{\sup}\Bigl[\mathcal{L}(w, \lambda)\Bigr] \\
    &= \begin{cases}
        c^Tx - \lambda^T\mathbf{1} & M^Tc + F^T\lambda = 0 \\
        \infty & \text{Otherwise}
    \end{cases}
\end{align*}
The Lagrangian dual function is an upper bound on the worst-case cost for all values of $\lambda \ge 0$, therefore the tightest upper bound on the worst-case cost can be expressed as the solution to the following optimisation problem:
\begin{align*}
    \underset{\lambda}{\text{Minimise}} \quad & c^Tx - \lambda^T\mathbf{1} \\
    \text{Subject to} \quad & M^Tc + F^T\lambda = 0 \\
    & \lambda \ge 0
\end{align*}
The original optimisation problem can now be expressed as minimising the tightest upper bound on the worst-case cost:
\begin{align*}
    \underset{M, x, \lambda}{\text{Minimise}} \quad & c^Tx - \lambda^T\mathbf{1} \\
    \text{Subject to} \quad & (AM - B)w \le b - Ax \quad (\forall w\in\mathcal{W}) \\
    & M^Tc + F^T\lambda = 0 \\
    & \lambda \ge 0
\end{align*}
The problem now requires finding bounds on each element of the vector $(AM - B)w$ for all $w\in\mathcal{W}$, which itself depends on the decision variable $M$, and the previously found constraints on $M$. The greatest value that the $i$th element of this vector can take can itself be expressed as the solution to another optimisation problem, where $a_i^T$ and $b_i^T$ refer to the $i$th rows of the matrices $A$ and $B$, respectively:
\begin{align*}
    \underset{w}{\text{Maximise}} \quad & (a_i^TM - b_i^T)w \\
    \text{Subject to} \quad & Fw \le \mathbf{1} \\
    & M^Tc + F^T\lambda = 0
\end{align*}
The Lagrangian and Langrangian dual functions for this problem can be defined as follows:
\begin{align*}
    \mathcal{L}_i(w, \mu_i, \nu_i) &= (a_i^TM - b_i^T)w + \mu_i^T(Fw - \mathbf{1}) + \nu_i^T(M^Tc + F^T\lambda) \\
    &= (M^Ta_i - b_i + F^T\mu_i)^Tw - \mu_i^T \mathbf{1} + \nu_i^T(M^Tc + F^T\lambda) \\
    g_i(\mu_i, \nu_i) &= \underset{w}{\sup}\Bigl[ \mathcal{L}_i(w, \mu_i, \nu_i) \Bigr] \\
    &= \begin{cases}
        -\mu_i^T \mathbf{1} + \nu_i^T(M^Tc + F^T\lambda) \quad & M^Ta_i - b_i + F^T\mu_i = 0 \\
        \infty & \text{Otherwise}
    \end{cases}
\end{align*}
The Lagrangian dual function is an upper bound on $(a_i^TM - b_i^T)w$ for all values of $\mu_i \ge 0$ and $\nu_i$, therefore the tightest upper bound on $(a_i^TM - b_i^T)w$ can be expressed as the solution to the following optimisation problem:
\begin{align*}
    \underset{\mu_i, \nu_i}{\text{Minimise}} \quad & -\mu_i^T \mathbf{1} + \nu_i^T(M^Tc + F^T\lambda) \\
    \text{Subject to} \quad & M^Ta_i - b_i + F^T\mu_i = 0 \\
    & \mu_i \ge 0
\end{align*}
At last, the solution to the original optimisation problem can be expressed as follows, wherein $[b]_i$ refers to the $i$th element of the vector $b$, to be distinguished from $b_i^T$ which refers to the $i$th row of $B$:
\begin{align*}
    \underset{M, x, \lambda, \{\mu_i\}_i, \{\nu_i\}_i}{\text{Minimise}} \quad & c^Tx - \lambda^T\mathbf{1} \\
    \text{Subject to} \quad & M^Tc + F^T\lambda = 0 \\
    & M^Ta_i - b_i + F^T\mu_i = 0 & (\forall i) \\
    & -\mu_i^T \mathbf{1} + \nu_i^T(M^Tc + F^T\lambda) \le [b]_i - a_i^Tx & (\forall i) \\
    & \mu_i \ge 0 & (\forall i) \\
    & \lambda \ge 0
\end{align*}
