Automatic differentiation refers to an approach to numerical programming in which operations on numbers (or arrays of numbers) are recorded in a computation graph, with the numbers being represented by nodes, and the operations on those numbers being represented by edges. This allows gradients of the results of those computations to be calculated automatically by applying the chain rule along the edges of the computation graph, assuming that the local derivative of each operation in the computation graph is known.

This approach is very powerful, and of particular relevance to the field of machine learning, because once a model and loss function have been expressed in terms of a computation graph, assuming the model and loss function are differentiable, parameters of the model can be optimised with respect to the loss function using gradient-based methods, in which the gradients are calculated using automatic differentiation, without having to derive the gradients explicitly. One popular framework for automatic differentiation is PyTorch \cite{paszke2019pytorch}, which was used during all the experiments described in this report.
