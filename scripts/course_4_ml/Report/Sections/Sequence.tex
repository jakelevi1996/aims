Neural networks such as the MLP can be adapted for sequence prediction simply by concatenating the input $x_t$ to the model at time $t$ with a hidden state $h_t$, using a neural network to calculate both an output from the model $y_t$ and a new hidden state $h_{t+1}$, and then using the new hidden state as the input hidden state for the next time step, when a new input/output pair is processed by the model. This type of model is known as a recurrent neural network (RNN). A particular type of recurrent neural network is the long short-term memory (LSTM) model \cite{hochreiter1997long}, which uses gates to learn to control the flow of information from one time step to the next. Using automatic differentiation makes it very straightforward (in principle) to optimise the parameters of a RNN, simply by defining a loss function for the network, averaging that loss function over time steps and over batches of training data within a time step, and then using the automatically computed gradients of the average loss function to optimise the parameters of the model.

Figure \ref{fig:Shakespeare} shows the loss functions over time while training both an RNN and an LSTM for 8 hours on the complete works of Shakespeare\footnote{Freely downloaded from \color{blue}{\href{https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt}{https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt}}}, with the objective of minimising the cross-entropy between the models' predictions of the next character and the actual next character, given a substring of text from the training data. This approach also allows the model to generate new strings of text from its predicted distribution over future characters, and an example of these predictions are included in appendix \ref{appendix:rnn samples}. Unfortunately, despite training for 8 hours, the models were not able to generate coherent text in the style of Shakespeare. Possible improvements to the models include looking at different parameter initialisation methods (because the parameters were initialised according to \cite{he2015delving}, which assumes relu activation functions, but the LSTM in particular uses sigmoid and $\tanh$ activation functions at the gate outputs), using dropout \cite{srivastava2014dropout} \cite{gal2016theoretically}, batch normalisation \cite{ioffe2015batch}, layer normalisation \cite{ba2016layer}, different optimisers (such as Adam \cite{kingma2014adam}), different network architectures, and other types of regularisation (such as weight decay).
