The Bellman equation for the state value function $V^\pi(s)$ can be expressed in matrix form as follows (note that here the expected reward $R(s',s,a)$ when transitioning to state $s'$ after performing action $a$ in state $s$ is used instead of the expected reward $r(s, a)$ when action $a$ is taken in state $s$, which is not conditioned on the next state):
\begin{align*}
    \underbrace{V^\pi(s)}_{v^\pi_s} &= \sum_{a\in\mathcal{A}}{\left[ \pi(a\vert s) \sum_{s'\in\mathcal{S}}{\left[ p(s'\vert s, a) \Bigl( R(s', s, a) + \gamma V^\pi(s') \Bigr) \right]} \right]} \\
    &= \underbrace{\sum_{a\in\mathcal{A}}{\left[ \pi(a\vert s) \sum_{s'\in\mathcal{S}}{\Bigl[ p(s'\vert s, a) R(s', s, a) \Bigr]} \right]}}_{r^\pi_s} + \gamma\sum_{a\in\mathcal{A}}{\left[ \pi(a\vert s) \sum_{s'\in\mathcal{S}}{\Bigl[ p(s'\vert s, a) V^\pi(s') \Bigr]} \right]} \\
    &= r^\pi_s + \gamma\sum_{s'\in\mathcal{S}}{\left[ \underbrace{\sum_{a\in\mathcal{A}}{\Bigl[ \pi(a\vert s) p(s'\vert s, a) \Bigr]}}_{P^\pi_{s, s'}} \underbrace{V^\pi(s')}_{v^\pi_{s'}} \right]} \\
    &= r^\pi_s + \gamma\sum_{s'\in\mathcal{S}}{\left[ P^\pi_{s, s'} v^\pi_{s'} \right]} \\
    &= r^\pi_s + \gamma  (P^\pi v^\pi)_s \\
    \Rightarrow \quad v^\pi &= r^\pi + \gamma P^\pi v^\pi \\
    \text{where} \quad & \begin{cases}
        v^\pi_i &= V^\pi(i) \\
        r^\pi_i &= \sum_{a\in\mathcal{A}}{\left[ \pi(a\vert i) \sum_{s'\in\mathcal{S}}{\Bigl[ p(s'\vert i, a) R(s', i, a) \Bigr]} \right]} \\
        P^\pi_{i, j} &= \sum_{a\in\mathcal{A}}{\Bigl[ \pi(a\vert i) p(j\vert i, a) \Bigr]} \\
        &= p(s_{t+1}=j\vert s_t=i)
    \end{cases}
\end{align*}
TODO: what is policy evaluation? Prove that it converges
