The Bellman equation for the state value function $V^\pi(s)$ can be expressed in matrix form as follows (note that here the expected reward $R(s',s,a)$ when transitioning to state $s'$ after performing action $a$ in state $s$ is used instead of the expected reward $r(s, a)$ when action $a$ is taken in state $s$, which is not conditioned on the next state):
\begin{align*}
    \underbrace{V^\pi(s)}_{v^\pi_s} &= \sum_{a\in\mathcal{A}}{\left[ \pi(a\vert s) \sum_{s'\in\mathcal{S}}{\left[ p(s'\vert s, a) \Bigl( R(s', s, a) + \gamma V^\pi(s') \Bigr) \right]} \right]} \\
    &= \underbrace{\sum_{a\in\mathcal{A}}{\left[ \pi(a\vert s) \sum_{s'\in\mathcal{S}}{\Bigl[ p(s'\vert s, a) R(s', s, a) \Bigr]} \right]}}_{r^\pi_s} + \gamma\sum_{a\in\mathcal{A}}{\left[ \pi(a\vert s) \sum_{s'\in\mathcal{S}}{\Bigl[ p(s'\vert s, a) V^\pi(s') \Bigr]} \right]} \\
    &= r^\pi_s + \gamma\sum_{s'\in\mathcal{S}}{\left[ \underbrace{\sum_{a\in\mathcal{A}}{\Bigl[ \pi(a\vert s) p(s'\vert s, a) \Bigr]}}_{P^\pi_{s, s'}} \underbrace{V^\pi(s')}_{v^\pi_{s'}} \right]} \\
    &= r^\pi_s + \gamma\sum_{s'\in\mathcal{S}}{\left[ P^\pi_{s, s'} v^\pi_{s'} \right]} \\
    &= r^\pi_s + \gamma  (P^\pi v^\pi)_s \\
    \Rightarrow \quad v^\pi &= r^\pi + \gamma P^\pi v^\pi \\
    \text{where} \quad & \begin{cases}
        v^\pi_i &= V^\pi(i) \\
        r^\pi_i &= \sum_{a\in\mathcal{A}}{\left[ \pi(a\vert i) \sum_{s'\in\mathcal{S}}{\Bigl[ p(s'\vert i, a) R(s', i, a) \Bigr]} \right]} \\
        P^\pi_{i, j} &= \sum_{a\in\mathcal{A}}{\Bigl[ \pi(a\vert i) p(j\vert i, a) \Bigr]} \\
        &= p(s_{t+1}=j\vert s_t=i)
    \end{cases}
\end{align*}
If the value of any or all states is unknown, but the transition probabilities and expected rewards are known, then the dynamic programming technique of policy evaluation can be used to estimate the values of unknown states. Policy evaluation refers to iteratively updating an estimate for the value of each state as follows:
\begin{align*}
    v_{k+1}^\pi(s) &= \mathbb{E}_\pi\left.\Bigl[ r_t + \gamma v_k^\pi(s_{t+1}) \middle\vert s_t=s \Bigr]\right. \\
    &= \sum_{a\in\mathcal{A}}{\left[ \pi(a\vert s) \left( r(s, a) + \gamma\sum_{s'\in\mathcal{S}}{\Bigl[ p(s'\vert s, a) v_k^\pi(s') \Bigr]} \right) \right]}
\end{align*}
For $\gamma\in[0,1)$ it can be proved that policy evaluation converges. The error in the estimate $v_k^\pi(s)$ of the state value function $V^\pi(s)$ of state $s$ during iteration $k$ of policy evaluation is denoted by $\varepsilon_k(s)$:
\begin{align*}
    (\forall s\in\mathcal{S})\quad&& v_k^\pi(s) &= V^\pi(s) + \varepsilon_k(s) \\
    \Rightarrow(\forall s\in\mathcal{S})\quad&& v_{k+1}^\pi(s) &= \sum_{a\in\mathcal{A}}{\left[ \pi(a\vert s) \left( r(s, a) + \gamma\sum_{s'\in\mathcal{S}}{\left[ p(s'\vert s, a) \Bigl( V^\pi(s') + \varepsilon_k(s') \Bigr) \right]} \right) \right]} \\
    &&&= \sum_{a\in\mathcal{A}}{\left[ \pi(a\vert s) \left( r(s, a) + \gamma\sum_{s'\in\mathcal{S}}{\Bigl[ p(s'\vert s, a) V^\pi(s') \Bigr]} + \gamma\sum_{s'\in\mathcal{S}}{\Bigl[ p(s'\vert s, a) \varepsilon_k(s') \Bigr]} \right) \right]} \\
    &&&= V^\pi(s) + \gamma\sum_{a\in\mathcal{A}}{\left[ \pi(a\vert s) \sum_{s'\in\mathcal{S}}{\Bigl[ p(s'\vert s, a) \varepsilon_k(s') \Bigr]} \right]} \\
    &&&\le V^\pi(s) + \gamma\sum_{a\in\mathcal{A}}{\left[ \pi(a\vert s) \sum_{s'\in\mathcal{S}}{\left[ p(s'\vert s, a) \max_{\bar{s}}\Bigl[\varepsilon_k(\bar{s})\Bigr] \right]} \right]} \\
    &&&= V^\pi(s) + \gamma\max_{\bar{s}}\Bigl[\varepsilon_k(\bar{s})\Bigr] \\
    (\forall s\in\mathcal{S})\quad&& v_{k+1}^\pi(s) &= V^\pi(s) + \varepsilon_{k+1}(s) \\
    \Rightarrow(\forall s\in\mathcal{S})\quad&& V^\pi(s) + \varepsilon_{k+1}(s) &\le V^\pi(s) + \gamma\max_{\bar{s}}\Bigl[\varepsilon_k(\bar{s})\Bigr] \\
    \Rightarrow(\forall s\in\mathcal{S})\quad&& \varepsilon_{k+1}(s) &\le \gamma\max_{\bar{s}}\Bigl[\varepsilon_k(\bar{s})\Bigr] \\
    \Rightarrow\quad&& \max_{\bar{s}}\Bigl[\varepsilon_k(\bar{s})\Bigr] &\le \gamma\max_{\bar{s}}\Bigl[\varepsilon_k(\bar{s})\Bigr]
\end{align*}
This implies that the upper bound across states of the error in the estimated state value function decreases at least by a factor of $\gamma$ on every iteration, and therefore decreases at least exponentially (similar reasoning can be used to derive the lower bound). As a result, a desired tolerance in the estimated state value function will always be reached if policy evaluation is run for sufficiently many iterations.
