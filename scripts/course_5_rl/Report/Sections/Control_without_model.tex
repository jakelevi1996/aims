Policy evaluation, policy iteration and value iteration are dynamic programming approaches which all assume access to an exact model of the environment, however in the general reinforcement learning problem such a model is not available. Two simple approaches to control without a model are constant-$\alpha$ Monte Carlo (MC) and Temporal Difference (TD) methods. MC estimates the return from a given state using samples of the reward from all subsequently observed states of an episode, whereas TD(0) estimates the return from a given state using the sample of the reward from that state, and the "bootstrapped" estimate of the return from the next state. The differences between how these algorithms estimate return leads to differences in the bias and variance of the updates that these algorithms perform.

MC methods have high variance, because the estimate of the return at each time step depends on many rewards which are sampled stochastically from the environment. MC methods also have low bias, because sampled rewards come from the true distribution of rewards in each state, and not an estimate of the reward distribution based on previous samples.

Conversely, TD(0) methods have low variance, because the estimate of the return at each time step only depends on a single stochastic sample of the reward. However, TD(0) methods also have high bias, because each estimate of the return depends on a previous estimate of the return from the subsequent state, and in general this estimate will not be fully accurate.

In summary, MC methods tend to have lower bias, but TD(0) methods tend to have lower variance.

When using such a method in practise, an $\epsilon$-greedy policy is useful because it ensures that in any given state, every action has a non-zero probability of being chosen. As a result, if these algorithms run for long enough, every state-action pair will be sampled enough times to reach a desired level of certainty in its value. If a policy was used in which some state-action pairs were never sampled, it could be the case that some states were never visited, so their value could not be accurately estimated, and the algorithm might never converge to an optimal policy. In general, a behaviour policy is the policy used when interacting with the environment, and the target policy is the policy whose value is being estimated, and these two policies need not be the same (for example in off-policy methods). To guarantee convergence of the \emph{target} policy to an optimal policy, it is necessary for the \emph{behaviour} policy to sample all state-action pairs with non-zero probability.

Q-learning is an alternative to MC and TD(0) methods, which estimates the optimal expected return when choosing any action in any given state, using the following update equation:
\begin{equation*}
    Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_t + \gamma \max_a \Bigl[ Q(s_{t+1}, a) \Bigr] - Q(s_t, a_t) \right)
\end{equation*}
Q-learning can be interpreted as estimating the value of a state-action pair according to a policy that only chooses optimal actions during future time steps, and that never chooses sub-optimal actions during future timesteps. Therefore, Q-learning can be interpreted as estimating the state-action value function of an optimal \emph{target} policy. In general, the behaviour policy will choose all actions in a given state with non-zero probability (for example using $\epsilon$-greedy choices) in order to guarantee convergence, which in general will be different to (and have lower value than) the target policy. Q-learning can therefore be considered an off-policy method because the policy whose value it estimates (the target policy) is different to the policy it follows in the environment (the behaviour policy).
