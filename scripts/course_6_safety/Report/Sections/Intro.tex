For decades, scientists and engineers have been trying to design Artificial Intelligence (AI) systems that match or exceed human intelligence. Although we are yet to see an example of a single AI system that can match or exceed all of the broad capabilities of human intelligence simultaneously, the achievements of AI systems have become increasingly more impressive over time. This leads one to consider what the consequences for humanity might be if and when someone does succeed in designing an AI system that exceeds human intelligence. It has been demonstrated \cite{cohen2022advanced} that a sufficiently powerful AI system could cause the extinction of humanity, if the agent has been programmed to maximise a reward signal that has not been properly designed. Therefore in the interest of our own self-preservation, it is worthwhile to consider how a reward signal for an AI agent can be designed in order to alleviate or minimise the risk of human extinction in the case of a sufficiently powerful AI agent. To this end, we first introduce the AIXI framework \cite{hutter2000theory}, followed by Inverse Reward Design \cite{hadfield2017inverse}, and then combine the two ideas and consider how together they could be used to minimise the risk of human extinction at the hands of a sufficiently powerful AI agent.
